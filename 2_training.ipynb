{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClimateHack.AI 2023: Training a Basic Model\n",
    "\n",
    "Thank you for participating in ClimateHack.AI 2023! \n",
    "\n",
    "Your contributions could help cut carbon emissions by up to 100 kilotonnes per year in Great Britain alone. We look forward to seeing what you build over the course of the competition!\n",
    "\n",
    "In this Jupyter notebook, you will hopefully train your first model for the challenge using historical solar PV data and HRV satellite imagery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing packages\n",
    "\n",
    "Before you can get started, you will need to install a number of packages to allow you to work with the data and submit to the platform. If you do not already have these packages installed, you can uncomment the lines below to do so! You will also need to [install PyTorch](https://pytorch.org/get-started/locally/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install zarr xarray gcsfs fsspec dask cartopy ocf-blosc2 torchinfo tqdm\n",
    "# %pip install lightning\n",
    "# %pip install -U doxa-cli\n",
    "# !git clone https://github.com/nhat-vo/getting-started-2023.git && mv getting-started-2023/* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages\n",
    "\n",
    "Here, we import a number of packages we will need to train our first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, time, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import xarray as xr\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torchinfo import summary\n",
    "import json\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import DeviceStatsMonitor\n",
    "from ocf_blosc2 import Blosc2\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading a month of data\n",
    "\n",
    "While streaming the Zarr-format datasets directly from Hugging Face was adequate for some initial data exploration in `1_data.ipynb`, it most likely will not be fast enough in training. Since there is so much data available, we can get started just by downloading a single month of PV and HRV satellite imagery data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data/pv/2020\", exist_ok=True)\n",
    "    os.makedirs(\"data/satellite-hrv/2020\", exist_ok=True)\n",
    "\n",
    "    !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/pv/metadata.csv --output data/pv/metadata.csv\n",
    "    !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/pv/2020/7.parquet --output data/pv/2020/7.parquet\n",
    "    !curl -L https://huggingface.co/datasets/climatehackai/climatehackai-2023/resolve/main/satellite-hrv/2020/7.zarr.zip --output data/satellite-hrv/2020/7.zarr.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv = pd.read_parquet(\"data/pv/2020/7.parquet\").drop(\"generation_wh\", axis=1)\n",
    "pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrv = xr.open_dataset( \"data/satellite-hrv/2020/7.zarr.zip\", engine=\"zarr\", chunks=\"auto\")\n",
    "hrv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the challenge, you can make use of satellite imagery, numerical weather prediction and air quality forecast data in a `[128, 128]` region centred on each solar PV site. In order to help you out, we have pre-computed the indices corresponding to each solar PV site and included them in `indices.json`, which we can load directly. For more information, take a look at the [challenge page](https://doxaai.com/competition/climatehackai-2023).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"indices.json\") as f:\n",
    "    blobs = json.load(f)\n",
    "    site_locations = {\n",
    "        v: {int(r): (int(blob[r][0]), int(blob[r][1])) for r in blob}\n",
    "        for v, blob in blobs.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a PyTorch Dataset\n",
    "\n",
    "To get started, we will define a simple `IterableDataset` that shows how to slice into the PV and HRV data using `pandas` and `xarray`, respectively. You will have to modify this if you wish to incorporate non-HRV data, weather forecasts and air quality forecasts into your training regimen. If you have any questions, feel free to ask on the [ClimateHack.AI Community Discord server](https://discord.gg/HTTQ8AFjJp)!\n",
    "\n",
    "**Note**: `site_locations` contains indices for the non-HRV, weather forecast and air quality forecast data as well as for the HRV data!\n",
    "\n",
    "There are many more advanced strategies you could implement to load data in training, particularly if you want to pre-prepare training batches in advance or use multiple workers to improve data loading times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChallengeDataset(IterableDataset):\n",
    "    def __init__(self, pv, hrv, site_locations, start_date, end_date):\n",
    "        super().__init__()\n",
    "        self.pv = pv\n",
    "        self.hrv = hrv\n",
    "        self._site_locations = site_locations\n",
    "\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.start_time = time(8)\n",
    "        self.end_time = time(17)\n",
    "\n",
    "    def _get_image_times(self):\n",
    "        date = self.start_date\n",
    "        while date < self.end_date:\n",
    "            current_time = datetime.combine(date, self.start_time)\n",
    "            while current_time.time() < self.end_time:\n",
    "                if current_time:\n",
    "                    yield current_time\n",
    "\n",
    "                current_time += timedelta(minutes=60)\n",
    "\n",
    "            date += timedelta(days=1)\n",
    "            \n",
    "    \n",
    "    # def __len__(self):\n",
    "    #     start = datetime.combine(self.start_date, self.start_time) \n",
    "    #     end = datetime.combine(self.end_date, self.end_time) \n",
    "    #     time_count = int((end - start).total_seconds()) // 3600\n",
    "    #     return time_count * len(self._site_locations[\"hrv\"])\n",
    "\n",
    "    def __iter__(self):\n",
    "        for time in self._get_image_times():\n",
    "            first_hour = slice(str(time), str(time + timedelta(minutes=55)))\n",
    "\n",
    "            pv_features = pv.xs(first_hour, drop_level=False)  # type: ignore\n",
    "            pv_targets = pv.xs(\n",
    "                slice(  # type: ignore\n",
    "                    str(time + timedelta(hours=1)),\n",
    "                    str(time + timedelta(hours=4, minutes=55)),\n",
    "                ),\n",
    "                drop_level=False,\n",
    "            )\n",
    "\n",
    "            hrv_data = self.hrv[\"data\"].sel(time=first_hour).to_numpy()\n",
    "            np.pad(hrv_data, ((0, 0), (64, 64), (64, 64), (0, 0)))\n",
    "\n",
    "            for site in self._site_locations[\"hrv\"]:\n",
    "                try:\n",
    "                    # Get solar PV features and targets\n",
    "                    site_features = pv_features.xs(site, level=1).to_numpy().squeeze(-1)\n",
    "                    site_targets = pv_targets.xs(site, level=1).to_numpy().squeeze(-1)\n",
    "                    assert site_features.shape == (12,) and site_targets.shape == (48,)\n",
    "\n",
    "                    # Get a 128x128 HRV crop centred on the site over the previous hour\n",
    "                    x, y = self._site_locations[\"hrv\"][site]\n",
    "                    hrv_features = hrv_data[:, y - 64 : y + 64, x - 64 : x + 64, 0]\n",
    "                    assert hrv_features.shape == (12, 128, 128)\n",
    "\n",
    "                    # How might you adapt this for the non-HRV, weather and aerosol data?\n",
    "                except:\n",
    "                    print('ignoring', x, y)\n",
    "                    continue\n",
    "\n",
    "                yield site_features, hrv_features, site_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id):\n",
    "    worker_info = torch.utils.data.get_worker_info()\n",
    "    dataset = worker_info.dataset  # the dataset copy in this worker process\n",
    "    overall_start = dataset.start_date\n",
    "    overall_end = dataset.end_date\n",
    "    days_count = (overall_end - overall_start).days\n",
    "\n",
    "    # configure the dataset to only process the split workload\n",
    "    per_worker = int(math.ceil(days_count / float(worker_info.num_workers)))\n",
    "    date_delta = timedelta(days=per_worker)\n",
    "    dataset.start_date = overall_start + worker_id * date_delta\n",
    "    dataset.end_date = min(dataset.start_date + date_delta, overall_end)\n",
    "    print(f\"Worker {worker_id} processing {dataset.start_date} to {dataset.end_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a model\n",
    "\n",
    "In order to make a PyTorch-based submission to the DOXA AI platform, you need to upload both the code defining your model in addition to your trained model weights (and some code to run your model). As a result, if you want to experiment with different model architectures using this notebook, you will need to edit the model in `submission/model.py` and re-import it here.\n",
    "\n",
    "Here is the small convolutional neural network you are initially given in `submission/model.py`. You will absolutely be able to improve upon this!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model defined in `submission/model.py`\n",
    "\n",
    "from submission.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelModule(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = nn.L1Loss()\n",
    "        # self.example_input_array = [torch.Tensor(1, 12), torch.Tensor(1, 12, 128, 128)]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        pv_features, hrv_features, pv_targets = batch\n",
    "        predictions = self.model(\n",
    "            pv_features,\n",
    "            hrv_features,\n",
    "        )\n",
    "\n",
    "        loss = self.criterion(predictions, pv_targets)\n",
    "\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        pv_features, hrv_features, pv_targets = batch\n",
    "        predictions = self.model(\n",
    "            pv_features,\n",
    "            hrv_features,\n",
    "        )\n",
    "\n",
    "        loss = self.criterion(predictions, pv_targets.to)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_data = ChallengeDataset(pv, hrv, site_locations=site_locations, start_date=datetime(2020, 7, 1), end_date=datetime(2020, 7, 25))\n",
    "val_data = ChallengeDataset(pv, hrv, site_locations=site_locations, start_date=datetime(2020, 7, 25), end_date=datetime(2020, 8, 1))\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelModule(Model())\n",
    "print(summary(model.model, input_size=[(1, 12), (1, 12, 128, 128)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "trainer = L.Trainer(accelerator=\"gpu\", max_epochs=EPOCHS, profiler=\"advanced\", callbacks=[DeviceStatsMonitor()])\n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your model\n",
    "torch.save(model.model.state_dict(), \"submission/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting to the DOXA AI platform\n",
    "\n",
    "Congratulations &ndash; **you have trained your first model for ClimateHack.AI 2023**! 🥳\n",
    "\n",
    "Why not try making a submission to the platform?\n",
    "\n",
    "First, make sure you have enrolled for the competition on the [ClimateHack.AI 2023 competition page](https://doxaai.com/competition/climatehackai-2023). You will need to be signed in with a DOXA AI account registered with your university email address so that we can verify your eligibility.\n",
    "\n",
    "You can then sign in with the CLI using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!doxa login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can upload your submission to the platform by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!doxa upload submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went well, you will soon appear on the [competition scoreboard](https://doxaai.com/competition/climatehackai-2023/scoreboard) once your model has been evaluated! 😎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Well done for reaching the end of this Jupyter notebook! By now, you will have loaded and explored the data, trained a basic model, and joined other competition participants on the [competition scoreboard](https://doxaai.com/competition/climatehackai-2023/scoreboard)!\n",
    "\n",
    "To get started, we used a very simple model architecture, but this model most likely does not have a sufficiently rich representation to properly solve our problem. How might you be able to improve on this? Which model architectures would be best suited to this problem? Would you want to train a model from scratch, as we have done here, or possibly fine-tune a pre-trained computer vision model? Check out the resources on the [competition page](https://doxaai.com/competition/climatehackai-2023) for ideas on where to go from here.\n",
    "\n",
    "Additionally, we only used historical PV and HRV data, but perhaps you might be able to get more mileage out of the other data sources available to you, such as non-HRV satellite imagery, the DWD weather forecast data or even the aerosol data. If you do decide to incorporate more data, what **data engineering** work would you have to perform so that you can train effectively on a large quantity of data?\n",
    "\n",
    "**We want to hear about your approaches**! If you develop anything interesting, let us know on the [ClimateHack.AI Community Discord server](https://discord.gg/HTTQ8AFjJp) and start a conversation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
